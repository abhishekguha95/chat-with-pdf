import requests
import json
import logging
from typing import Iterator, List, Dict, Any
from ..config import config

# Set up logger for this module
logger = logging.getLogger(__name__)


class OllamaService:
    """
    Service for interacting with Ollama LLM API.

    This class handles communication with a locally running Ollama server
    to generate responses based on provided context and user queries.
    It supports streaming responses for better user experience.
    """

    def __init__(self):
        """
        Initialize the Ollama service with configuration from the app config.

        Sets up the base URL for the Ollama API and the model to use for text generation.
        """
        # Remove trailing slash from host URL if present for consistent URL building
        self.base = config.OLLAMA_HOST.rstrip("/")
        self.model = config.OLLAMA_MODEL

    def _build_prompt(
        self, context: str, query: str, history: List[Dict[str, Any]] | None
    ) -> str:
        """
        Construct a well-formatted prompt for the LLM that includes context, history, and the query.

        The prompt structure is designed to provide clear instructions to the model about:
        1. Its role as a helpful assistant
        2. How to handle questions outside the provided context
        3. Previous conversation history for continuity
        4. The relevant document context for the current query
        5. The user's current question

        Args:
            context: Document chunks retrieved as context for answering
            query: The user's current question
            history: List of previous messages in the conversation

        Returns:
            A formatted prompt string ready to be sent to the LLM
        """
        parts = []
        # Define the assistant's role and behavior for out-of-context questions
        parts.append(
            "You are a helpful assistant that answers using only the provided context. "
            "If the answer is not in the context, reply: "
            "'I cannot find that information in the provided documents.'"
        )

        # Include recent conversation history if available (limited to last 5 messages)
        if history:
            parts.append("\nPrevious conversation:")
            for m in history[-5:]:
                role = "Human" if m.get("role") == "user" else "Assistant"
                parts.append(f"{role}: {m.get('content','')}")

        # Add retrieved document context
        if context:
            parts.append("\nContext:\n" + context)

        # Add the current user question and prompt for an answer
        parts.append("\nQuestion: " + query)
        parts.append("Answer:")

        # Combine all parts into a complete prompt
        return "\n".join(parts)

    def stream_completion(
        self,
        context_text: str,
        query: str,
        chat_history: List[Dict[str, Any]] | None = None,
    ) -> Iterator[str]:
        """
        Generate a streaming completion from the LLM based on context and query.

        This method:
        1. Constructs a prompt combining context, history, and query
        2. Makes a streaming request to the Ollama API
        3. Yields generated tokens as they become available

        Args:
            context_text: Relevant document context for answering
            query: User's question
            chat_history: Previous messages in the conversation

        Yields:
            Text tokens as they're generated by the model

        Raises:
            RuntimeError: If the Ollama API returns an error
            requests.exceptions.RequestException: For connection/timeout issues
        """
        # Build the full prompt with context, history and query
        prompt = self._build_prompt(context_text, query, chat_history)

        # Configure request payload with model parameters
        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": True,  # Enable streaming response
            "options": {
                "temperature": 0.7,  # Controls randomness (0.0 = deterministic, 1.0 = creative)
                "top_p": 0.9,  # Nucleus sampling parameter
                "stop": [
                    "Human:",
                    "Question:",
                ],  # Stop generation when these strings are encountered
            },
        }

        try:
            # Make streaming request to Ollama API
            r = requests.post(
                f"{self.base}/api/generate",
                json=payload,
                stream=True,
                timeout=60,  # 60-second timeout for the request
            )

            # Check for API errors
            if r.status_code != 200:
                raise RuntimeError(f"Ollama error {r.status_code}: {r.text}")

            # Process the streaming response
            for line in r.iter_lines():
                if not line:
                    continue
                try:
                    # Parse JSON from the stream
                    data = json.loads(line.decode("utf-8"))
                    if "response" in data:
                        tok = data["response"]
                        if tok:
                            yield tok  # Yield each token as it arrives
                    # Stop when the model indicates it's finished
                    if data.get("done", False):
                        break
                except json.JSONDecodeError:
                    continue
        except requests.exceptions.RequestException as e:
            logger.error(f"Ollama request failed: {e}")
            raise
        except Exception as e:
            logger.error(f"Streaming failed: {e}")
            raise

    def health_check(self) -> bool:
        """
        Check if the Ollama API is accessible and responding.

        This method makes a lightweight request to the Ollama tags endpoint
        to verify that the service is up and running.

        Returns:
            bool: True if the service is healthy, False otherwise
        """
        try:
            resp = requests.get(f"{self.base}/api/tags", timeout=5)
            return resp.status_code == 200
        except Exception:
            return False


# Create a singleton instance of the service for use throughout the application
llm_service = OllamaService()
